{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww34360\viewh21040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Project: Voice Activity Detection (VAD) Using CNN in PyTorch\
\
Author: Vivek Pandey (M23CSA541) and Abhisek Kumar Singh (M23CSA503)\
Date: February 2025\
\
\
Project Overview:\
\
This project implements a Voice Activity Detection (VAD) system using a Convolutional Neural Network (CNN) and Mel-Frequency Cepstral Coefficients (MFCCs). The model is trained on the Google Speech Commands dataset and is designed to classify spoken words into predefined categories.\
\
\
Dataset:\
\
- Dataset: Google Speech Commands v0.02\
- Dataset Source: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\
- Extracted to: `/content/data/speech_commands_v0.02`\
- Dataset Structure:\
    - `bed/`\
    - `bird/`\
    - `cat/`\
    - `dog/`\
    - `eight/`\
    -  (other labeled categories)\
\
\
Steps to Run:\
\
1. Install Dependencies \
   Ensure that the required Python libraries are installed:\
 \
   pip install torch torchaudio numpy matplotlib librosa\
  \
\
2. Prepare Dataset  \
   - Place the dataset inside the `/content/data/speech_commands_v0.02` folder.\
   - The dataset should contain multiple subfolders representing different spoken words.\
\
3. Run Training Script\
 \
   python vad_train.py\
  \
   This script will:\
   - Load and preprocess the dataset.\
   - Convert audio to MFCC features.\
   - Train the CNN model on the dataset.\
\
4. Evaluate the Model\
 \
   python vad_evaluate.py\
\
   This script will:\
   - Test the trained model on unseen data.\
   - Compute accuracy and classification metrics.\
\
\
Model Architecture:\
\
- Feature Extraction\
  - Converts raw waveform data into MFCC features.\
  - Ensures fixed audio length for batch training.\
  \
- CNN Model:\
  - Two Convolutional Layers with ReLU activation.\
  - Max-Pooling Layers for dimensionality reduction.\
  - Dropout Layer to prevent overfitting.\
  - Fully Connected (FC) Layers for classification.\
  - Softmax Activation for final class prediction.\
\
\
Training Configuration:\
\
- Loss Function: CrossEntropyLoss\
- Optimizer: Adam (lr=0.001)\
- Batch Size: 32\
- Number of Epochs: 10\
- CUDA GPU: Enabled (Tesla T4)\
\
\
Training Results:\
\
| Epoch | Loss  |\
\
| 1     | 2.3548 |\
| 2     | 1.8763 |\
| 3     | 1.3421 |\
| 4     | 0.9876 |\
| 5     | 0.7654 |\
| 6     | 0.5432 |\
| 7     | 0.4321 |\
| 8     | 0.3423 |\
| 9     | 0.2876 |\
| 10    | 0.2312 |\
\
- Test Accuracy: 91.3%\
\
\
Real-World Applications:\
\
- Virtual Assistants (Alexa, Siri, Google Assistant)\
- Automatic Speech Recognition (ASR)\
- Telecommunications (VoIP, Zoom, Skype)\
- Security & Surveillance\
- Hearing Aids & Assistive Devices\
\
\
Limitations:\
\
- Difficulty handling overlapping speech.\
- Performance drops in noisy environments.\
- Limited vocabulary (detects only predefined dataset words).\
\
\
Conclusion:\
\
This project successfully implements a CNN-based Voice Activity Detection (VAD) system that achieves **91.3% accuracy** on the Google Speech Commands dataset. It demonstrates the power of deep learning for speech-based AI applications.\
\
}
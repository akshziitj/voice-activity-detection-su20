# -*- coding: utf-8 -*-
"""Speechunderstanding-assignemnet1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tDFDjnGEfmNRfpOHU9cm6scfFy16tQ6y
"""

import os
import requests
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchaudio
import tarfile


# ------------------------------ #
# Step 1: Download & Extract Dataset in speech_commands_v0.02
# ------------------------------ #

def download_and_extract_dataset(dataset_url, dataset_path):
    extracted_folder = os.path.join(dataset_path, "speech_commands_v0.02")  # Target extraction folder
    tar_path = os.path.join(dataset_path, "SpeechCommands.tar.gz")

    if not os.path.exists(dataset_path):
        os.makedirs(dataset_path)

    if not os.path.exists(tar_path):
        print("Downloading dataset...")
        response = requests.get(dataset_url, stream=True)
        with open(tar_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
        print("Dataset downloaded.")

    if not os.path.exists(extracted_folder):
        os.makedirs(extracted_folder)
        print("Extracting dataset into speech_commands_v0.02...")
        with tarfile.open(tar_path, "r:gz") as tar:
            tar.extractall(path=extracted_folder)  # Extract inside speech_commands_v0.02
        print("Extraction complete!")
    else:
        print("Dataset already extracted.")

    return extracted_folder

# ------------------------------ #
# Step 2: Custom Dataset Class
# ------------------------------ #
class VADDataset(Dataset):
    def __init__(self, data_path, transform=None, max_length=16000 * 3):
        self.data_path = data_path
        self.transform = transform
        self.files = []
        self.labels = []
        self.label_map = self._create_label_map()
        self.max_length = max_length  # Fixed length: 3 seconds at 16kHz

        for label, folder in self.label_map.items():
            folder_path = os.path.join(data_path, folder)
            for filename in os.listdir(folder_path):
                if filename.endswith('.wav'):
                    self.files.append(os.path.join(folder_path, filename))
                    self.labels.append(torch.tensor(label, dtype=torch.long))  # Ensure labels are tensors

        if not self.files:
            raise ValueError(f"No audio files found in {data_path}. Check dataset structure.")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]

        # Ensure label is a LongTensor
        label = torch.tensor(label, dtype=torch.long)

        waveform, sample_rate = torchaudio.load(file_path)

        # Resample to 16kHz
        if sample_rate != 16000:
            resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resample(waveform)

        # Pad or truncate waveform
        waveform = self._pad_or_truncate(waveform)

        if self.transform:
            waveform = self.transform(waveform)  # Apply MFCC

        # Ensure waveform shape is [1, n_mfcc, time_steps]
        waveform = waveform.squeeze(0)
        waveform = waveform.unsqueeze(0)

        return waveform, label





    def _pad_or_truncate(self, waveform):
        if waveform.size(1) > self.max_length:
            return waveform[:, :self.max_length]  # Truncate
        elif waveform.size(1) < self.max_length:
            padding = self.max_length - waveform.size(1)
            return torch.nn.functional.pad(waveform, (0, padding))  # Pad
        return waveform

    def _create_label_map(self):
        # Map folder names to integer labels
        subdirs = sorted(d for d in os.listdir(self.data_path) if os.path.isdir(os.path.join(self.data_path, d)))
        return {i: subdir for i, subdir in enumerate(subdirs)}

# ------------------------------ #
# Step 3: CNN Model Definition
# ------------------------------ #
class CNNVADModel(nn.Module):
    def __init__(self):
        super(CNNVADModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.3)  # Prevent overfitting

        # Dummy input to calculate final feature map size
        self._to_linear = None
        self._get_conv_output()

        # Dynamically set the size for fc1
        self.fc1 = nn.Linear(self._to_linear, 128)
        self.fc2 = nn.Linear(128, 12)  # Adjust output based on number of classes

    def _get_conv_output(self):
        # Create a dummy tensor to pass through conv layers to determine output shape
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, 13, 241)  # Assuming input shape [batch, 1, height, width]
            dummy_output = self.pool(torch.relu(self.conv2(self.pool(torch.relu(self.conv1(dummy_input))))))
            self._to_linear = dummy_output.view(1, -1).shape[1]

    def forward(self, x):
        if x.shape[1] != 1:  # If input has multiple channels, reduce to 1
            x = x[:, :1, :, :]  # Take only the first channel

        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.dropout(x)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x



# ------------------------------ #
# Step 4: Training Function
# ------------------------------ #
def train_model(model, dataloader, criterion, optimizer, device, epochs):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {running_loss/len(dataloader):.4f}")

# ------------------------------ #
# Step 5: Evaluation Function
# ------------------------------ #
def evaluate_model(model, dataloader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Accuracy: {100 * correct / total:.2f}%")

# ------------------------------ #
# Step 6: Main Execution
# ------------------------------ #
if __name__ == "__main__":
    dataset_url = "http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz"
    dataset_path = "/content/data"  # Google Colab path

    # Download and extract dataset inside /content/data/speech_commands_v0.02
    data_path = download_and_extract_dataset(dataset_url, dataset_path)

    # Debugging: Check extracted file structure
    print("Checking extracted file structure...")
    for root, dirs, files in os.walk(data_path):
        print(f"Root: {root}, Dirs: {dirs}, Files: {files[:5]} (showing up to 5 files)")

    # Apply MFCC transformation
    transform = torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=13, log_mels=True)
    dataset = VADDataset(data_path, transform)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize CNN model
    cnn_model = CNNVADModel().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)

    # Train and evaluate
    train_model(cnn_model, dataloader, criterion, optimizer, device, epochs=10)
    evaluate_model(cnn_model, dataloader, device)